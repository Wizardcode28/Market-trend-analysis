# -*- coding: utf-8 -*-
"""Untitled41.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u4xuHja5UQtlHQdq5JahJmXSKarzqTDY
"""



# --- Install if missing ---
# !pip install tensorflow scikit-learn matplotlib pandas ipywidgets

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.metrics import mean_absolute_error, mean_squared_error
import ipywidgets as widgets
from IPython.display import display
import math

# ======================
# Load dataset
# ======================
csv_path = "/content/infolimpioavanzadoTarget.csv"  # <-- upload file in Colab
df = pd.read_csv(csv_path)

# Clean column names
df.columns = [c.strip().lower() for c in df.columns]

# --- Dropdown for company selection ---
companies = df['ticker'].unique().tolist()
dropdown = widgets.Dropdown(
    options=companies,
    description='Select company:',
    style={'description_width': 'initial'},
    layout={'width': '50%'}
)
display(dropdown)

# ======================
# Function to train + forecast
# ======================
def run_lstm_for_company(company_name, lookback=60, horizon=30, epochs=20):
    # Filter data for selected company
    df_c = df[df['ticker'] == company_name].copy().reset_index(drop=True)

    # Keep only numeric columns
    num = df_c.select_dtypes(include=[np.number])
    num = num.replace([np.inf, -np.inf], np.nan).fillna(method="ffill").fillna(method="bfill")

    # Features (all except 'close'), Target ('close')
    X_all = num.drop(columns=['close']).values.astype('float32')
    y_all = num['close'].values.astype('float32').reshape(-1,1)

    # Scaling
    x_scaler = StandardScaler()
    X_scaled = x_scaler.fit_transform(X_all)
    y_scaler = MinMaxScaler()
    y_scaled = y_scaler.fit_transform(y_all)

    # Sequence builder
    def make_sequences(X, y, lookback, horizon):
        Xs, ys = [], []
        for i in range(len(X) - lookback - horizon):
            Xs.append(X[i:i+lookback])
            ys.append(y[i+lookback:i+lookback+horizon].flatten())
        return np.array(Xs), np.array(ys)

    X_seq, y_seq = make_sequences(X_scaled, y_scaled, lookback, horizon)

    if len(X_seq) < 10:
        print(f"âš ï¸ Not enough data for {company_name} to build sequences.")
        return

    # Train/Test split
    split = int(0.8 * len(X_seq))
    X_train, y_train = X_seq[:split], y_seq[:split]
    X_test, y_test = X_seq[split:], y_seq[split:]

    # Deep LSTM Model
    model = Sequential([
        LSTM(256, return_sequences=True, input_shape=(lookback, X_seq.shape[2])),
        Dropout(0.3),
        LSTM(128, return_sequences=True),
        Dropout(0.3),
        LSTM(64),
        Dropout(0.3),
        Dense(horizon)  # Predict next horizon close values
    ])

    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss="mse", metrics=["mae"])

    # Train
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=32,
        verbose=1
    )

    # Plot training history
    plt.figure(figsize=(8,4))
    plt.plot(history.history['loss'], label='train loss')
    plt.plot(history.history['val_loss'], label='val loss')
    plt.legend(); plt.show()

    # Evaluate on test
    y_pred_test = model.predict(X_test)
    mae = mean_absolute_error(y_test.flatten(), y_pred_test.flatten())
    rmse = math.sqrt(mean_squared_error(y_test.flatten(), y_pred_test.flatten()))
    print(f"ðŸ“Š Test MAE: {mae:.4f}, RMSE: {rmse:.4f}")

    # Forecast next horizon days
    X_last = X_scaled[-lookback:, :][None, ...]
    y_pred_scaled = model.predict(X_last)[0]
    y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1,1)).flatten()

    print(f"\nðŸ”® Next {horizon}-day forecast for {company_name}:")
    print(y_pred)

    # Plot: last 60 real closes vs. next horizon forecast
    plt.figure(figsize=(12,5))
    plt.plot(range(len(y_all[-60:])), y_all[-60:], label="Actual (last 60 days)", marker='o')
    plt.plot(range(len(y_all[-1:]), len(y_all[-1:]) + horizon), y_pred, label=f"Forecast (next {horizon} days)", marker='x')
    plt.title(f"{company_name} Forecast vs Actual")
    plt.xlabel("Time steps")
    plt.ylabel("Close Price")
    plt.legend()
    plt.show()

# ======================
# Run once company is selected
# ======================
def on_change(change):
    if change['type'] == 'change' and change['name'] == 'value':
        run_lstm_for_company(change['new'], lookback=60, horizon=30, epochs=20)

dropdown.observe(on_change)

# --- Install if missing ---
# !pip install tensorflow scikit-learn matplotlib pandas ipywidgets

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.metrics import mean_absolute_error, mean_squared_error
import ipywidgets as widgets
from IPython.display import display
import math

# ======================
# Load dataset
# ======================
csv_path = "/content/infolimpioavanzadoTarget.csv"  # <-- upload file in Colab
df = pd.read_csv(csv_path)

# Clean column names
df.columns = [c.strip().lower() for c in df.columns]

# --- Dropdown for company selection ---
companies = df['ticker'].unique().tolist()
dropdown = widgets.Dropdown(
    options=companies,
    description='Select company:',
    style={'description_width': 'initial'},
    layout={'width': '50%'}
)
display(dropdown)

# ======================
# Function to train + forecast
# ======================
def run_lstm_for_company(company_name, lookback=60, horizon=30, epochs=20):
    # Filter data for selected company
    df_c = df[df['ticker'] == company_name].copy().reset_index(drop=True)

    # Keep only numeric columns
    num = df_c.select_dtypes(include=[np.number])
    num = num.replace([np.inf, -np.inf], np.nan).fillna(method="ffill").fillna(method="bfill")

    # Features (all except 'close'), Target ('close')
    X_all = num.drop(columns=['close']).values.astype('float32')
    y_all = num['close'].values.astype('float32').reshape(-1,1)

    # Scaling
    x_scaler = StandardScaler()
    X_scaled = x_scaler.fit_transform(X_all)
    y_scaler = MinMaxScaler()
    y_scaled = y_scaler.fit_transform(y_all)

    # Sequence builder
    def make_sequences(X, y, lookback):
        Xs, ys = [], []
        for i in range(len(X) - lookback):
            Xs.append(X[i:i+lookback])
            ys.append(y[i+lookback])
        return np.array(Xs), np.array(ys)

    X_seq, y_seq = make_sequences(X_scaled, y_scaled, lookback)

    if len(X_seq) < 10:
        print(f"âš ï¸ Not enough data for {company_name} to build sequences.")
        return

    # Train/Test split
    split = int(0.8 * len(X_seq))
    X_train, y_train = X_seq[:split], y_seq[:split]
    X_test, y_test = X_seq[split:], y_seq[split:]

    # Deep LSTM Model
    model = Sequential([
        LSTM(256, return_sequences=True, input_shape=(lookback, X_seq.shape[2])),
        Dropout(0.3),
        LSTM(128, return_sequences=True),
        Dropout(0.3),
        LSTM(64),
        Dropout(0.3),
        Dense(1)  # predict next 1-day close
    ])

    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss="mse", metrics=["mae"])

    # Train
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=32,
        verbose=1
    )

    # Plot training history
    plt.figure(figsize=(8,4))
    plt.plot(history.history['loss'], label='train loss')
    plt.plot(history.history['val_loss'], label='val loss')
    plt.legend(); plt.show()

    # Evaluate on test
    y_pred_test = model.predict(X_test)
    y_pred_test_inv = y_scaler.inverse_transform(y_pred_test)
    y_test_inv = y_scaler.inverse_transform(y_test)

    mae = mean_absolute_error(y_test_inv.flatten(), y_pred_test_inv.flatten())
    rmse = math.sqrt(mean_squared_error(y_test_inv.flatten(), y_pred_test_inv.flatten()))
    print(f"ðŸ“Š Test MAE: {mae:.4f}, RMSE: {rmse:.4f}")

    # Recursive Forecasting (multi-step)
    X_last = X_scaled[-lookback:, :].copy()  # last window
    preds = []
    for _ in range(horizon):
        X_input = X_last.reshape(1, lookback, -1)
        y_pred_scaled = model.predict(X_input, verbose=0)
        y_pred = y_scaler.inverse_transform(y_pred_scaled)[0,0]
        preds.append(y_pred)

        # Update window: drop first row, append predicted close (scaled)
        new_row = X_last[-1].copy()
        new_row[ num.drop(columns=['close']).shape[1]-1 ] = y_pred_scaled  # replace last feature with new close
        X_last = np.vstack([X_last[1:], new_row])

    preds = np.array(preds)

    print(f"\nðŸ”® Next {horizon}-day forecast for {company_name}:")
    print(preds)

    # Plot: last 60 real closes vs. next horizon forecast
    plt.figure(figsize=(12,5))
    plt.plot(range(len(y_all[-60:])), y_all[-60:], label="Actual (last 60 days)", marker='o')
    plt.plot(range(len(y_all[-1:]), len(y_all[-1:]) + horizon), preds, label=f"Forecast (next {horizon} days)", marker='x')
    plt.title(f"{company_name} Forecast vs Actual")
    plt.xlabel("Time steps")
    plt.ylabel("Close Price")
    plt.legend()
    plt.show()

# ======================
# Run once company is selected
# ======================
def on_change(change):
    if change['type'] == 'change' and change['name'] == 'value':
        run_lstm_for_company(change['new'], lookback=60, horizon=30, epochs=20)

dropdown.observe(on_change)



# ============================================
# Stock Price Prediction with Stacked BiLSTM
# ============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout
from ta.momentum import RSIIndicator
from ta.trend import SMAIndicator
from sklearn.metrics import mean_absolute_error, mean_squared_error
import math

# ======================
# Load Dataset
# ======================
df = pd.read_csv("infolimpioavanzadoTarget.csv")

# Ensure correct column names (adjust if your dataset differs)
df = df[['date','open','high','low','close','volume']]
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)


# ======================
# Feature Engineering
# ======================
# RSI (Relative Strength Index)
df['RSI'] = RSIIndicator(close=df['close'], window=14).rsi()

# Moving Averages
df['MA20'] = SMAIndicator(close=df['close'], window=20).sma_indicator()
df['MA50'] = SMAIndicator(close=df['close'], window=50).sma_indicator()

df = df.dropna()

# ======================
# Scaling
# ======================
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)

# ======================
# Sequence Generator
# ======================
lookback = 120   # Increased window
forecast_horizon = 30   # Predict 30 days

def create_sequences(data, lookback, horizon):
    X, y = [], []
    for i in range(lookback, len(data)-horizon):
        X.append(data[i-lookback:i])         # past lookback days
        y.append(data[i:i+horizon, 3])       # future close prices (column 3 = 'Close')
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_data, lookback, forecast_horizon)

# Train-Test Split
split = int(len(X)*0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

print("Train shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, y_test.shape)

# ======================
# Build Model
# ======================
model = Sequential([
    Bidirectional(LSTM(128, return_sequences=True), input_shape=(lookback, X.shape[2])),
    Dropout(0.2),
    Bidirectional(LSTM(64, return_sequences=False)),
    Dropout(0.2),
    Dense(forecast_horizon)   # Output next 30 days Close prices
])

model.compile(optimizer='adam', loss='mse')
history = model.fit(X_train, y_train, epochs=30, batch_size=32,
                    validation_split=0.1, verbose=1)

# ======================
# Evaluation
# ======================
pred_scaled = model.predict(X_test)
# Inverse transform
y_test_rescaled, pred_rescaled = [], []
for i in range(len(pred_scaled)):
    temp = np.zeros((forecast_horizon, scaled_data.shape[1]))
    temp[:,3] = y_test[i]
    y_test_rescaled.append(scaler.inverse_transform(temp)[:,3])

    temp[:,3] = pred_scaled[i]
    pred_rescaled.append(scaler.inverse_transform(temp)[:,3])

y_test_rescaled, pred_rescaled = np.array(y_test_rescaled), np.array(pred_rescaled)

# Metrics (compare only first forecasted day for fairness)
mae = mean_absolute_error(y_test_rescaled[:,0], pred_rescaled[:,0])
rmse = math.sqrt(mean_squared_error(y_test_rescaled[:,0], pred_rescaled[:,0]))
print("Test MAE:", mae)
print("Test RMSE:", rmse)

# ======================
# Plot last test prediction vs actual
# ======================
plt.figure(figsize=(12,6))
plt.plot(y_test_rescaled[-1], label="Actual Future")
plt.plot(pred_rescaled[-1], label="Predicted Future")
plt.title("30-Day Forecast")
plt.xlabel("Days Ahead")
plt.ylabel("Price")
plt.legend()
plt.show()

# ======================
# Forecast Next 30 Days
# ======================
last_sequence = scaled_data[-lookback:]
last_sequence = np.expand_dims(last_sequence, axis=0)
future_pred_scaled = model.predict(last_sequence)

temp = np.zeros((forecast_horizon, scaled_data.shape[1]))
temp[:,3] = future_pred_scaled[0]
future_pred = scaler.inverse_transform(temp)[:,3]

plt.figure(figsize=(12,6))
plt.plot(future_pred, label="Next 30-Day Forecast")
plt.title("Future Stock Price Forecast")
plt.xlabel("Days Ahead")
plt.ylabel("Price")
plt.legend()
plt.show()

# Use lowercase column names
df = df[['date','open','high','low','close','volume']]
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

# ============================================
# Stock Price Prediction with Stacked BiLSTM
# ============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from ta.momentum import RSIIndicator
from ta.trend import SMAIndicator
from sklearn.metrics import mean_absolute_error, mean_squared_error
import math

# ======================
# Load Dataset
# ======================
df = pd.read_csv("infolimpioavanzadoTarget.csv")

# Clean column names
df.columns = [c.strip().lower() for c in df.columns]

# ======================
# Main Prediction Function
# ======================
def run_forecast_for_company(company_name):
    """
    Filters the dataset for a given company, trains a BiLSTM model,
    and generates a 30-day stock price forecast.
    """
    print(f"Starting prediction for {company_name}...")

    # Filter data for selected company
    df_c = df[df['ticker'] == company_name].copy().reset_index(drop=True)

    if df_c.empty:
        print(f"âš ï¸ No data found for company: {company_name}")
        return

    # Keep only required columns
    df_c = df_c[['date', 'open', 'high', 'low', 'close', 'volume']]
    df_c['date'] = pd.to_datetime(df_c['date'])
    df_c.set_index('date', inplace=True)

    # ======================
    # Feature Engineering
    # ======================
    # RSI (Relative Strength Index)
    df_c['RSI'] = RSIIndicator(close=df_c['close'], window=14).rsi()

    # Moving Averages
    df_c['MA20'] = SMAIndicator(close=df_c['close'], window=20).sma_indicator()
    df_c['MA50'] = SMAIndicator(close=df_c['close'], window=50).sma_indicator()

    df_c = df_c.dropna()

    if len(df_c) < 150:  # Check if enough data is available after dropping NaNs
        print(f"âš ï¸ Not enough data points for {company_name} after feature engineering. Found {len(df_c)} rows.")
        return

    # ======================
    # Scaling
    # ======================
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df_c)

    # ======================
    # Sequence Generator
    # ======================
    lookback = 120   # Increased window
    forecast_horizon = 30   # Predict 30 days

    def create_sequences(data, lookback, horizon):
        X, y = [], []
        # Find the index of the 'close' column in the scaled data
        close_col_index = df_c.columns.get_loc('close')

        for i in range(lookback, len(data) - horizon):
            X.append(data[i-lookback:i])         # past lookback days
            y.append(data[i:i+horizon, close_col_index])  # future close prices
        return np.array(X), np.array(y)

    X, y = create_sequences(scaled_data, lookback, forecast_horizon)

    if len(X) == 0:
        print(f"âš ï¸ Not enough data for {company_name} to create sequences with a lookback of {lookback} and horizon of {forecast_horizon}.")
        return

    # Train-Test Split
    split = int(len(X) * 0.8)
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    print(f"Train shape: {X_train.shape}, {y_train.shape}")
    print(f"Test shape: {X_test.shape}, {y_test.shape}")

    # ======================
    # Build & Compile Model
    # ======================
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=(lookback, X.shape[2])),
        Dropout(0.3),
        Bidirectional(LSTM(32, return_sequences=False)),
        Dropout(0.3),
        Dense(forecast_horizon)
    ])

    model.compile(optimizer='adam', loss='mse')

    # Implement Early Stopping to prevent overfitting
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=5,
        mode='min',
        restore_best_weights=True
    )

    # Train Model with Early Stopping
    history = model.fit(
        X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.1,
        callbacks=[early_stopping],
        verbose=1
    )

    # Plot training history to visualize overfitting
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Model Loss Over Epochs for {company_name}')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # ======================
    # Evaluation
    # ======================
    pred_scaled = model.predict(X_test)

    # Inverse transform
    y_test_rescaled, pred_rescaled = [], []
    close_col_index = df_c.columns.get_loc('close')
    for i in range(len(pred_scaled)):
        temp = np.zeros((forecast_horizon, scaled_data.shape[1]))
        temp[:, close_col_index] = y_test[i]
        y_test_rescaled.append(scaler.inverse_transform(temp)[:, close_col_index])

        temp[:, close_col_index] = pred_scaled[i]
        pred_rescaled.append(scaler.inverse_transform(temp)[:, close_col_index])

    y_test_rescaled, pred_rescaled = np.array(y_test_rescaled), np.array(pred_rescaled)

    # Metrics (compare only first forecasted day for fairness)
    mae = mean_absolute_error(y_test_rescaled[:,0], pred_rescaled[:,0])
    rmse = math.sqrt(mean_squared_error(y_test_rescaled[:,0], pred_rescaled[:,0]))
    print(f"\nTest MAE: {mae:.4f}")
    print(f"Test RMSE: {rmse:.4f}")

    # ======================
    # Plot last test prediction vs actual
    # ======================
    plt.figure(figsize=(12,6))
    plt.plot(y_test_rescaled[-1], label="Actual Future")
    plt.plot(pred_rescaled[-1], label="Predicted Future")
    plt.title(f"30-Day Forecast vs. Actual for {company_name}")
    plt.xlabel("Days Ahead")
    plt.ylabel("Price")
    plt.legend()
    plt.show()

    # ======================
    # Forecast Next 30 Days
    # ======================
    last_sequence = scaled_data[-lookback:]
    last_sequence = np.expand_dims(last_sequence, axis=0)
    future_pred_scaled = model.predict(last_sequence)

    temp = np.zeros((forecast_horizon, scaled_data.shape[1]))
    temp[:, close_col_index] = future_pred_scaled[0]
    future_pred = scaler.inverse_transform(temp)[:, close_col_index]

    plt.figure(figsize=(12,6))
    plt.plot(future_pred, label="Next 30-Day Forecast")
    plt.title(f"Future Stock Price Forecast for {company_name}")
    plt.xlabel("Days Ahead")
    plt.ylabel("Price")
    plt.legend()
    plt.show()

# ======================
# User Input and Execution
# ======================
# Get unique company tickers
tickers = df['ticker'].unique()
print("Available company tickers:", tickers)

# Prompt user for company input
company_ticker = input("Enter a company ticker from the list to start the forecast: ").upper()

# Run the forecast for the selected company
if company_ticker in tickers:
    run_forecast_for_company(company_ticker)
else:
    print(f"Invalid ticker: {company_ticker}. Please run the code again and choose from the list.")

# ============================================
# Stock Price Prediction with Stacked BiLSTM
# ============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from ta.momentum import RSIIndicator
from ta.trend import SMAIndicator
from sklearn.metrics import mean_absolute_error, mean_squared_error
import math

# ======================
# Load Dataset
# ======================
df = pd.read_csv("infolimpioavanzadoTarget.csv")

# Clean column names
df.columns = [c.strip().lower() for c in df.columns]

# ======================
# Main Prediction Function
# ======================
def run_forecast_for_company(company_name):
    """
    Filters the dataset for a given company, trains a BiLSTM model,
    and generates a 30-day stock price forecast.
    """
    print(f"Starting prediction for {company_name}...")

    # Filter data for selected company
    df_c = df[df['ticker'] == company_name].copy().reset_index(drop=True)

    if df_c.empty:
        print(f"âš ï¸ No data found for company: {company_name}")
        return

    # Keep only required columns
    df_c = df_c[['date', 'open', 'high', 'low', 'close', 'volume']]
    df_c['date'] = pd.to_datetime(df_c['date'])
    df_c.set_index('date', inplace=True)

    # ======================
    # Feature Engineering
    # ======================
    # RSI (Relative Strength Index)
    df_c['RSI'] = RSIIndicator(close=df_c['close'], window=14).rsi()

    # Moving Averages
    df_c['MA20'] = SMAIndicator(close=df_c['close'], window=20).sma_indicator()
    df_c['MA50'] = SMAIndicator(close=df_c['close'], window=50).sma_indicator()

    df_c = df_c.dropna()

    if len(df_c) < 150:  # Check if enough data is available after dropping NaNs
        print(f"âš ï¸ Not enough data points for {company_name} after feature engineering. Found {len(df_c)} rows.")
        return

    # ======================
    # Scaling
    # ======================
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df_c)

    # ======================
    # Sequence Generator
    # ======================
    lookback = 120   # Increased window
    forecast_horizon = 30   # Predict 30 days

    def create_sequences(data, lookback, horizon):
        X, y = [], []
        # Find the index of the 'close' column in the scaled data
        close_col_index = df_c.columns.get_loc('close')

        for i in range(lookback, len(data) - horizon):
            X.append(data[i-lookback:i])         # past lookback days
            y.append(data[i:i+horizon, close_col_index])  # future close prices
        return np.array(X), np.array(y)

    X, y = create_sequences(scaled_data, lookback, forecast_horizon)

    if len(X) == 0:
        print(f"âš ï¸ Not enough data for {company_name} to create sequences with a lookback of {lookback} and horizon of {forecast_horizon}.")
        return

    # Train-Test Split
    split = int(len(X) * 0.8)
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    print(f"Train shape: {X_train.shape}, {y_train.shape}")
    print(f"Test shape: {X_test.shape}, {y_test.shape}")

    # ======================
    # Build & Compile Model
    # ======================
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=(lookback, X.shape[2])),
        Dropout(0.3),
        Bidirectional(LSTM(32, return_sequences=False)),
        Dropout(0.3),
        Dense(forecast_horizon)
    ])

    model.compile(optimizer='adam', loss='mse')

    # Implement Early Stopping to prevent overfitting
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=5,
        mode='min',
        restore_best_weights=True
    )

    # Train Model with Early Stopping
    history = model.fit(
        X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.1,
        callbacks=[early_stopping],
        verbose=1
    )

    # Plot training history to visualize overfitting
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Model Loss Over Epochs for {company_name}')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # ======================
    # Evaluation
    # ======================
    pred_scaled = model.predict(X_test)

    # Inverse transform
    y_test_rescaled, pred_rescaled = [], []
    close_col_index = df_c.columns.get_loc('close')
    for i in range(len(pred_scaled)):
        temp = np.zeros((forecast_horizon, scaled_data.shape[1]))
        temp[:, close_col_index] = y_test[i]
        y_test_rescaled.append(scaler.inverse_transform(temp)[:, close_col_index])

        temp[:, close_col_index] = pred_scaled[i]
        pred_rescaled.append(scaler.inverse_transform(temp)[:, close_col_index])

    y_test_rescaled, pred_rescaled = np.array(y_test_rescaled), np.array(pred_rescaled)

    # Metrics (compare only first forecasted day for fairness)
    mae = mean_absolute_error(y_test_rescaled[:,0], pred_rescaled[:,0])
    rmse = math.sqrt(mean_squared_error(y_test_rescaled[:,0], pred_rescaled[:,0]))
    print(f"\nTest MAE: {mae:.4f}")
    print(f"Test RMSE: {rmse:.4f}")

    # ======================
    # Plot last test prediction vs actual
    # ======================
    plt.figure(figsize=(12,6))
    plt.plot(y_test_rescaled[-1], label="Actual Future")
    plt.plot(pred_rescaled[-1], label="Predicted Future")
    plt.title(f"30-Day Forecast vs. Actual for {company_name}")
    plt.xlabel("Days Ahead")
    plt.ylabel("Price")
    plt.legend()
    plt.show()

    # ======================
    # Forecast Next 30 Days
    # ======================
    last_sequence = scaled_data[-lookback:]
    last_sequence = np.expand_dims(last_sequence, axis=0)
    future_pred_scaled = model.predict(last_sequence)

    temp = np.zeros((forecast_horizon, scaled_data.shape[1]))
    temp[:, close_col_index] = future_pred_scaled[0]
    future_pred = scaler.inverse_transform(temp)[:, close_col_index]

    plt.figure(figsize=(12,6))
    plt.plot(future_pred, label="Next 30-Day Forecast")
    plt.title(f"Future Stock Price Forecast for {company_name}")
    plt.xlabel("Days Ahead")
    plt.ylabel("Price")
    plt.legend()
    plt.show()

# ======================
# User Input and Execution
# ======================
# Get unique company tickers
tickers = df['ticker'].unique()
print("Available company tickers:", tickers)

# Prompt user for company input
company_ticker = input("Enter a company ticker from the list to start the forecast: ").upper()

# Run the forecast for the selected company
if company_ticker in tickers:
    run_forecast_for_company(company_ticker)
else:
    print(f"Invalid ticker: {company_ticker}. Please run the code again and choose from the list.")

# ============================================
# Stock Price Prediction with Stacked BiLSTM
# ============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from ta.momentum import RSIIndicator
from ta.trend import SMAIndicator
from sklearn.metrics import mean_absolute_error, mean_squared_error
import math

# ======================
# Load Dataset
# ======================
df = pd.read_csv("infolimpioavanzadoTarget.csv")

# Clean column names
df.columns = [c.strip().lower() for c in df.columns]

# ======================
# Main Prediction Function
# ======================
def run_forecast_for_company(company_name):
    """
    Filters the dataset for a given company, trains a BiLSTM model,
    and generates a 30-day stock price forecast.
    """
    print(f"Starting prediction for {company_name}...")

    # Filter data for selected company
    df_c = df[df['ticker'] == company_name].copy().reset_index(drop=True)

    if df_c.empty:
        print(f"âš ï¸ No data found for company: {company_name}")
        return

    # Keep only required columns
    df_c = df_c[['date', 'open', 'high', 'low', 'close', 'volume']]
    df_c['date'] = pd.to_datetime(df_c['date'])
    df_c.set_index('date', inplace=True)

    # ======================
    # Feature Engineering
    # ======================
    # RSI (Relative Strength Index)
    df_c['RSI'] = RSIIndicator(close=df_c['close'], window=14).rsi()

    # Moving Averages
    df_c['MA20'] = SMAIndicator(close=df_c['close'], window=20).sma_indicator()
    df_c['MA50'] = SMAIndicator(close=df_c['close'], window=50).sma_indicator()

    df_c = df_c.dropna()

    if len(df_c) < 150:  # Check if enough data is available after dropping NaNs
        print(f"âš ï¸ Not enough data points for {company_name} after feature engineering. Found {len(df_c)} rows.")
        return

    # ======================
    # Scaling
    # ======================
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df_c)

    # ======================
    # Sequence Generator
    # ======================
    lookback = 120   # Increased window
    forecast_horizon = 30   # Predict 30 days

    def create_sequences(data, lookback, horizon):
        X, y = [], []
        # Find the index of the 'close' column in the scaled data
        close_col_index = df_c.columns.get_loc('close')

        for i in range(lookback, len(data) - horizon):
            X.append(data[i-lookback:i])         # past lookback days
            y.append(data[i:i+horizon, close_col_index])  # future close prices
        return np.array(X), np.array(y)

    X, y = create_sequences(scaled_data, lookback, forecast_horizon)

    if len(X) == 0:
        print(f"âš ï¸ Not enough data for {company_name} to create sequences with a lookback of {lookback} and horizon of {forecast_horizon}.")
        return

    # Train-Test Split
    split = int(len(X) * 0.8)
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    print(f"Train shape: {X_train.shape}, {y_train.shape}")
    print(f"Test shape: {X_test.shape}, {y_test.shape}")

    # ======================
    # Build & Compile Model
    # ======================
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=(lookback, X.shape[2])),
        Dropout(0.3),
        Bidirectional(LSTM(32, return_sequences=False)),
        Dropout(0.3),
        Dense(forecast_horizon)
    ])

    model.compile(optimizer='adam', loss='mse')

    # Implement Early Stopping to prevent overfitting
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=5,
        mode='min',
        restore_best_weights=True
    )

    # Train Model with Early Stopping
    history = model.fit(
        X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.1,
        callbacks=[early_stopping],
        verbose=1
    )

    # Plot training history to visualize overfitting
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Model Loss Over Epochs for {company_name}')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # ======================
    # Evaluation
    # ======================
    pred_scaled = model.predict(X_test)

    # Inverse transform
    y_test_rescaled, pred_rescaled = [], []
    close_col_index = df_c.columns.get_loc('close')
    for i in range(len(pred_scaled)):
        temp = np.zeros((forecast_horizon, scaled_data.shape[1]))
        temp[:, close_col_index] = y_test[i]
        y_test_rescaled.append(scaler.inverse_transform(temp)[:, close_col_index])

        temp[:, close_col_index] = pred_scaled[i]
        pred_rescaled.append(scaler.inverse_transform(temp)[:, close_col_index])

    y_test_rescaled, pred_rescaled = np.array(y_test_rescaled), np.array(pred_rescaled)

    # Metrics (compare only first forecasted day for fairness)
    mae = mean_absolute_error(y_test_rescaled[:,0], pred_rescaled[:,0])
    rmse = math.sqrt(mean_squared_error(y_test_rescaled[:,0], pred_rescaled[:,0]))
    print(f"\nTest MAE: {mae:.4f}")
    print(f"Test RMSE: {rmse:.4f}")

    # ======================
    # Plot last test prediction vs actual
    # ======================
    plt.figure(figsize=(12,6))
    plt.plot(y_test_rescaled[-1], label="Actual Future")
    plt.plot(pred_rescaled[-1], label="Predicted Future")
    plt.title(f"30-Day Forecast vs. Actual for {company_name}")
    plt.xlabel("Days Ahead")
    plt.ylabel("Price")
    plt.legend()
    plt.show()

    # ======================
    # Forecast Next 30 Days
    # ======================
    last_sequence = scaled_data[-lookback:]
    last_sequence = np.expand_dims(last_sequence, axis=0)
    future_pred_scaled = model.predict(last_sequence)

    temp = np.zeros((forecast_horizon, scaled_data.shape[1]))
    temp[:, close_col_index] = future_pred_scaled[0]
    future_pred = scaler.inverse_transform(temp)[:, close_col_index]

    plt.figure(figsize=(12,6))
    plt.plot(future_pred, label="Next 30-Day Forecast")
    plt.title(f"Future Stock Price Forecast for {company_name}")
    plt.xlabel("Days Ahead")
    plt.ylabel("Price")
    plt.legend()
    plt.show()

# ======================
# User Input and Execution
# ======================
# Get unique company tickers
tickers = df['ticker'].unique()
print("Available company tickers:", tickers)

# Prompt user for company input
company_ticker = input("Enter a company ticker from the list to start the forecast: ").upper()

# Run the forecast for the selected company
if company_ticker in tickers:
    run_forecast_for_company(company_ticker)
else:
    print(f"Invalid ticker: {company_ticker}. Please run the code again and choose from the list.")# ======================
# Save the Trained Model and Scaler
# ======================
if 'trained_model' in locals():
    model_filename = f"{company_ticker}_stock_model.joblib"
    try:
        joblib.dump({
            'model': trained_model,
            'scaler': data_scaler,
            'columns': data_columns
        }, model_filename)
        print(f"\nâœ… Model and scaler successfully saved to {model_filename}")
    except Exception as e:
        print(f"\nâŒ Failed to save model: {e}")



# ======================
# Save the Trained Model and Scaler
# ======================
if 'trained_model' in locals():
    model_filename = f"{company_ticker}_stock_model.pkl"
    try:
        joblib.dump({
            'model': trained_model,
            'scaler': data_scaler,
            'columns': data_columns
        }, model_filename)
        print(f"\nâœ… Model and scaler successfully saved to {model_filename}")
    except Exception as e:
        print(f"\nâŒ Failed to save model: {e}")



pip install skorch

pip install ta

"""
train_lstm_target_clean.py
--------------------------
- Trains LSTM on infolimpioavanzadoTarget.csv
- Handles NaN/inf/extreme values before scaling
- Saves weights (.pth) + bundle (.pkl) with scaler, features, params
"""
# THIS IS ALSO THE MAIN SCRIPT ISME MODEL TRAIN HUA HAI
import os
import json
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler

# ----------------------------
# Config
# ----------------------------
CSV_PATH = "infolimpioavanzadoTarget.csv"   # dataset file
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

LOOKBACK = 30
BATCH_SIZE = 64
EPOCHS = 20
LR = 1e-3
PATIENCE = 3

HIDDEN_DIM = 64
NUM_LAYERS = 2
DROPOUT = 0.3

LABEL_COL = "target"
TICKER_COL = "ticker"
SAVE_PLOT = True


# ----------------------------
# Dataset
# ----------------------------
class SeqDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# ----------------------------
# Model
# ----------------------------
class LSTMClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers=1, dropout=0.0):
        super().__init__()
        self.lstm = nn.LSTM(
            input_dim, hidden_dim, num_layers=num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0.0
        )
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]
        out = self.fc(out)
        return self.sigmoid(out).squeeze(-1)


# ----------------------------
# Data utils
# ----------------------------
def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Remove weird values globally"""
    df = df.copy()
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna(how="all")
    return df


def select_features(df: pd.DataFrame):
    """Select numeric columns except label/ticker"""
    features = [
        c for c in df.columns
        if c not in [LABEL_COL, TICKER_COL] and np.issubdtype(df[c].dtype, np.number)
    ]
    return df[features].copy(), features


def build_sequences_per_ticker(df, X_scaled, y, lookback, ticker_codes):
    """Build rolling windows, reset per ticker"""
    X_seq, y_seq = [], []
    for t in np.unique(ticker_codes):
        mask = ticker_codes == t
        X_t = X_scaled[mask]
        y_t = y[mask]
        for i in range(len(X_t) - lookback):
            X_seq.append(X_t[i:i+lookback])
            y_seq.append(y_t[i+lookback])
    return np.array(X_seq), np.array(y_seq)


def time_split(X, y, ratio=0.8):
    n = len(X)
    split = int(n * ratio)
    return X[:split], y[:split], X[split:], y[split:]


# ----------------------------
# Training utils
# ----------------------------
def train_model(model, train_loader, val_loader, epochs, lr, patience, device):
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    history = {"train_loss": [], "val_loss": []}
    best_val = float("inf")
    no_improve = 0

    model.to(device)
    for ep in range(epochs):
        model.train()
        tr_loss = 0
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            pred = model(xb)
            loss = criterion(pred, yb)
            loss.backward()
            optimizer.step()
            tr_loss += loss.item() * len(xb)
        tr_loss /= len(train_loader.dataset)

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for xb, yb in val_loader:
                xb, yb = xb.to(device), yb.to(device)
                pred = model(xb)
                loss = criterion(pred, yb)
                val_loss += loss.item() * len(xb)
        val_loss /= len(val_loader.dataset)

        history["train_loss"].append(tr_loss)
        history["val_loss"].append(val_loss)
        print(f"Epoch {ep+1}/{epochs} - train {tr_loss:.4f} - val {val_loss:.4f}")

        if val_loss < best_val:
            best_val = val_loss
            no_improve = 0
            torch.save(model.state_dict(), os.path.join(MODEL_DIR, "best.pth"))
        else:
            no_improve += 1
            if no_improve >= patience:
                print("Early stopping!")
                break

    model.load_state_dict(torch.load(os.path.join(MODEL_DIR, "best.pth")))
    return model, history


def evaluate_model(model, val_loader, device="cpu"):
    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

    model.eval()
    y_true, y_prob = [], []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.to(device)
            prob = model(xb).cpu().numpy()
            y_true.extend(yb.numpy())
            y_prob.extend(prob)

    y_true = np.array(y_true)
    y_prob = np.array(y_prob)
    y_pred = (y_prob >= 0.5).astype(int)

    return {
        "acc": float(accuracy_score(y_true, y_pred)),
        "f1": float(f1_score(y_true, y_pred)),
        "auc": float(roc_auc_score(y_true, y_prob))
    }, y_true, y_prob


# ----------------------------
# Main
# ----------------------------
def main():
    # Load & clean
    df = pd.read_csv(CSV_PATH)
    df = clean_dataframe(df)

    # --- Define features and target ---
    feature_cols = [
        'open', 'high', 'low', 'close', 'adjclose',
        'volume', 'rsiadjclose15', 'rsivolume15', 'rsiad'
    ]
    target_col = 'target'

    # Create target from next-day return
    df['return'] = df.groupby('ticker')['close'].pct_change().shift(-1)  # next-day return
    df['target'] = (df['return'] > 0).astype(int)

    # Drop NaN rows caused by shift
    df = df.dropna(subset=['target']).reset_index(drop=True)

    # keep only valid labels
    df = df[pd.to_numeric(df[LABEL_COL], errors="coerce").notna()].copy()
    df[LABEL_COL] = df[LABEL_COL].astype(np.float32)
    # features
    X_df, features = select_features(df)

    # clean features before scaling
    X_df = X_df.replace([np.inf, -np.inf], np.nan)
    X_df = X_df.dropna()
    X_df = X_df.clip(lower=-1e6, upper=1e6)

    df = df.loc[X_df.index]

    # scale
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X_df.values)

    y = df[LABEL_COL].values.astype(np.float32)
    tickers, ticker_codes = np.unique(df[TICKER_COL].astype(str).values, return_inverse=True)

    X_seq, y_seq = build_sequences_per_ticker(df, X_scaled, y, LOOKBACK, ticker_codes)
    if len(X_seq) == 0:
        raise RuntimeError("No sequences built, reduce LOOKBACK or check data.")

    X_tr, y_tr, X_va, y_va = time_split(X_seq, y_seq, 0.8)

    train_loader = DataLoader(SeqDataset(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=False)
    val_loader = DataLoader(SeqDataset(X_va, y_va), batch_size=BATCH_SIZE, shuffle=False)

    input_dim = X_seq.shape[2]
    model = LSTMClassifier(input_dim, HIDDEN_DIM, NUM_LAYERS, DROPOUT)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("Device:", device)

    model, history = train_model(model, train_loader, val_loader, EPOCHS, LR, PATIENCE, device)
    metrics, y_true, y_prob = evaluate_model(model, val_loader, device)
    print("Validation:", json.dumps(metrics, indent=2))

    # save weights + bundle
    torch.save(model.state_dict(), os.path.join(MODEL_DIR, "model.pth"))
    bundle = {
        "model_class": "LSTMClassifier",
        "model_kwargs": {"input_dim": int(input_dim), "hidden_dim": HIDDEN_DIM,
                         "num_layers": NUM_LAYERS, "dropout": DROPOUT},
        "scaler": scaler,
        "features": features,
        "lookback": LOOKBACK,
        "label_col": LABEL_COL,
        "metrics_val": metrics,
    }
    joblib.dump(bundle, os.path.join(MODEL_DIR, "bundle.pkl"))

    print("âœ… Saved model.pth and bundle.pkl in", MODEL_DIR)

    if SAVE_PLOT:
        plt.plot(history["train_loss"], label="train")
        plt.plot(history["val_loss"], label="val")
        plt.legend()
        plt.title("Loss curves")
        plt.savefig(os.path.join(MODEL_DIR, "loss.png"))
        plt.close()


if __name__ == "__main__":
    main()

# main.py
# ============================================
# Stock Price Prediction API with BiLSTM (TensorFlow) + Smart Datasource
# ============================================. # THIS IS THE MAIN CODE YEHI TERE MAIN JAISI FILE MAI CONVERT KIYA THA

from fastapi import FastAPI, Query
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import numpy as np
import os
from dotenv import load_dotenv
import requests
from typing import Optional
import logging
import traceback

# ML / Metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error

# Deep Learning (TensorFlow)
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Persist scaler and metadata
import joblib

# Optional Yahoo Finance
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False
    print("âš ï¸ yfinance not installed. Will rely on Alpha Vantage and CSV fallback.")

# ============================================
# ENV & Logging
# ============================================
load_dotenv()
API_KEY = os.getenv("MY_SECRET_API_KEY")
ALPHA_VANTAGE_API_KEY = os.getenv("ALPHA_VANTAGE_API_KEY", API_KEY)
AV_BASE_URL = "https://www.alphavantage.co/query"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("stock-bilstm")

# ============================================
# FastAPI App & CORS
# ============================================
app = FastAPI(
    title="ðŸ“ˆ Stock Price Prediction API (BiLSTM)",
    description="Predict stock prices using a stacked BiLSTM with smart datasource selection (Yahoo Finance for India, Alpha Vantage for US; CSV fallback).",
    version="2.0.0",
)
origins = [
    "https://market-trend-analysis.vercel.app",
    "http://localhost:8080",
    "http://127.0.0.1:8080",
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # tighten in production
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# ============================================
# Utils: Indicators (no 'ta' dependency)
# ============================================
def calculate_rsi(prices: pd.Series, period: int = 14) -> pd.Series:
    delta = prices.diff()
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)
    avg_gain = gain.rolling(period).mean()
    avg_loss = loss.rolling(period).mean()
    rs = avg_gain / avg_loss.replace(0, np.nan)
    rsi = 100 - (100 / (1 + rs))
    return rsi.fillna(50.0)

def add_features(df: pd.DataFrame) -> pd.DataFrame:
    # expects columns: open, high, low, close, volume
    out = df.copy()
    out["rsi"]  = calculate_rsi(out["close"], 14)
    out["ma20"] = out["close"].rolling(20).mean()
    out["ma50"] = out["close"].rolling(50).mean()
    out = out.dropna()
    return out

# ============================================
# Datasource: Alpha Vantage and Yahoo choice
# ============================================
def is_indian_symbol(symbol: str) -> bool:
    s = symbol.upper()
    return s.endswith(".NS") or s.endswith(".BO")

def fetch_from_alphavantage(symbol: str, timeout: int = 12) -> pd.DataFrame:
    params = {
        "function": "TIME_SERIES_DAILY_ADJUSTED",
        "symbol": symbol,
        "outputsize": "full",
        "apikey": ALPHA_VANTAGE_API_KEY
    }
    resp = requests.get(AV_BASE_URL, params=params, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    if "Time Series (Daily)" not in data:
        msg = data.get("Note") or data.get("Error Message") or "Alpha Vantage response missing time series."
        raise ValueError(msg)
    df = pd.DataFrame.from_dict(data["Time Series (Daily)"], orient="index").astype(float)
    df.index = pd.to_datetime(df.index)
    df = df.sort_index()
    # Normalize columns to simple names
    df = df.rename(columns={
        "1. open": "open",
        "2. high": "high",
        "3. low": "low",
        "4. close": "close",
        "5. adjusted close": "adj_close",
        "6. volume": "volume"
    })
    keep = ["open", "high", "low", "close", "volume"]
    df = df[keep]
    return df

def fetch_from_yfinance(symbol: str, period: str = "5y") -> pd.DataFrame:
    if not YFINANCE_AVAILABLE:
        raise ValueError("yfinance not installed")
    df = yf.download(symbol, period=period, progress=False)
    if df.empty:
        raise ValueError("yfinance returned empty data")
    df = df.rename(columns={
        "Open": "open",
        "High": "high",
        "Low": "low",
        "Close": "close",
        "Adj Close": "adj_close",
        "Volume": "volume",
    })
    keep = ["open", "high", "low", "close", "volume"]
    df = df[keep]
    df.index = pd.to_datetime(df.index)
    df = df.sort_index()
    return df


logger = logging.getLogger(__name__)

def fetch_stock_data(symbol: str) -> pd.DataFrame:
    last_exception = None

    # 1ï¸âƒ£ Try Yahoo Finance
    try:
        import yfinance as yf
        df = yf.download(symbol, period="6mo", interval="1d")
        if not df.empty:
            df.reset_index(inplace=True)
            df = df.rename(columns=str.lower).set_index("date")
            df = df[["open", "high", "low", "close", "volume"]]
            logger.info("Fetched %s data from Yahoo Finance", symbol)
            return df
    except Exception as e:
        logger.warning("Yahoo fetch failed for %s: %s", symbol, e)
        last_exception = e

    # 2ï¸âƒ£ Try AlphaVantage
    try:
        df = fetch_from_alphavantage(symbol)  # assumes this function exists
        logger.info("Fetched %s data from AlphaVantage", symbol)
        return df
    except Exception as e:
        logger.warning("AlphaVantage fetch failed for %s: %s", symbol, e)
        last_exception = e

    # 3ï¸âƒ£ Try CSV fallback
    csv_path = "infolimpioavanzadoTarget.csv"
    if os.path.exists(csv_path):
        try:
            df_csv = pd.read_csv(csv_path)
            df_csv.columns = [c.strip().lower() for c in df_csv.columns]
            df_csv = df_csv[df_csv["ticker"].str.upper() == symbol.upper()].copy()
            df_csv["date"] = pd.to_datetime(df_csv["date"])
            df_csv = df_csv.sort_values("date").set_index("date")
            df_csv = df_csv[["open", "high", "low", "close", "volume"]]
            logger.info("Using CSV fallback for %s", symbol)
            return df_csv
        except Exception as e:
            last_exception = e

    # 4ï¸âƒ£ Try bundle.pkl fallback
    bundle_path = "/mnt/data/bundle.pkl"
    if os.path.exists(bundle_path):
        try:
            with open(bundle_path, "rb") as f:
                bundle = pickle.load(f)
            # Assume bundle is a dict with symbols as keys and DataFrames as values
            if symbol.upper() in bundle:
                df_bundle = bundle[symbol.upper()]
                logger.info("Using bundle.pkl fallback for %s", symbol)
                return df_bundle
        except Exception as e:
            last_exception = e

    # 5ï¸âƒ£ Optional: Try model.pth (if it contains data, not weights)
    model_path = "/mnt/data/model.pth"
    if os.path.exists(model_path):
        try:
            model_data = torch.load(model_path, map_location="cpu")
            # If model_data contains a DataFrame or dict of DataFrames keyed by symbols
            if isinstance(model_data, dict) and symbol.upper() in model_data:
                df_model = model_data[symbol.upper()]
                logger.info("Using model.pth fallback for %s", symbol)
                return df_model
        except Exception as e:
            last_exception = e

    # âŒ If all fail
    raise ValueError(f"All sources failed for {symbol}. Last exception: {last_exception}")
# ============================================
# Sequences & Model
# ============================================
def make_sequences(scaled_array: np.ndarray, close_idx: int, lookback: int, horizon: int):
    X, y = [], []
    for i in range(lookback, len(scaled_array) - horizon):
        X.append(scaled_array[i - lookback:i])
        y.append(scaled_array[i:i + horizon, close_idx])
    X = np.asarray(X, dtype=np.float32)
    y = np.asarray(y, dtype=np.float32)
    return X, y

def build_bilstm(input_dim: int, lookback: int, horizon: int) -> tf.keras.Model:
    model = Sequential([
        Bidirectional(LSTM(64, return_sequences=True), input_shape=(lookback, input_dim)),
        Dropout(0.3),
        Bidirectional(LSTM(32, return_sequences=False)),
        Dropout(0.3),
        Dense(horizon)
    ])
    model.compile(optimizer="adam", loss="mse")
    return model

def train_or_load_model(symbol: str, df_raw: pd.DataFrame, lookback=120, horizon=30, force_retrain=False):
    """
    Returns (model, scaler, meta_dict, metrics_dict)
    Saves/loads from models_tf/{SYMBOL}_bilstm.h5 + _scaler.pkl
    """
    os.makedirs("models_tf", exist_ok=True)
    model_path = f"models_tf/{symbol}_bilstm.h5"
    scaler_path = f"models_tf/{symbol}_scaler.pkl"

    # Prepare data
    df = add_features(df_raw)  # adds rsi, ma20, ma50
    if len(df) < lookback + horizon + 50:
        raise ValueError(f"Not enough rows after feature engineering (got {len(df)}).")

    # Scale ALL columns used by sequences
    cols = ["open", "high", "low", "close", "volume", "rsi", "ma20", "ma50"]
    df_w = df[cols].copy()
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(df_w.values)
    close_idx = cols.index("close")

    X, y = make_sequences(scaled, close_idx, lookback, horizon)
    if len(X) == 0:
        raise ValueError("Not enough sequences created; try smaller lookback/horizon")

    # time-based split
    split = int(len(X) * 0.8)
    X_train, X_val = X[:split], X[split:]
    y_train, y_val = y[:split], y[split:]

    # Try load
    if os.path.exists(model_path) and os.path.exists(scaler_path) and not force_retrain:
        try:
            model = load_model(model_path)
            meta = {"cols": cols, "close_idx": close_idx, "lookback": lookback, "horizon": horizon}
            saved_scaler = joblib.load(scaler_path)
            # metrics from quick validation
            val_pred = model.predict(X_val, verbose=0)
            # inverse transform first horizon day to compute metrics
            mae, r2, rmse = _first_horizon_metrics(y_val, val_pred, scaler, close_idx, scaled.shape[1])
            metrics = {"mae": mae, "r2": r2, "rmse": rmse}
            return model, saved_scaler, meta, metrics
        except Exception as e:
            logger.warning("Failed to load saved model, retraining: %s", e)

    # Train fresh
    model = build_bilstm(input_dim=X.shape[2], lookback=lookback, horizon=horizon)
    early = EarlyStopping(monitor="val_loss", patience=5, mode="min", restore_best_weights=True)
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=50,
        batch_size=32,
        callbacks=[early],
        verbose=0
    )

    # Save artifacts
    model.save(model_path)
    joblib.dump({"scaler": scaler, "cols": cols}, scaler_path)

    # Compute metrics on validation (first horizon day)
    val_pred = model.predict(X_val, verbose=0)
    mae, r2, rmse = _first_horizon_metrics(y_val, val_pred, scaler, close_idx, scaled.shape[1])
    metrics = {"mae": mae, "r2": r2, "rmse": rmse}

    return model, scaler, {"cols": cols, "close_idx": close_idx, "lookback": lookback, "horizon": horizon}, metrics

def _first_horizon_metrics(y_true_scaled, y_pred_scaled, scaler, close_idx, n_features):
    # inverse-transform only the close column safely
    true_1d, pred_1d = [], []
    for i in range(len(y_pred_scaled)):
        temp = np.zeros((y_pred_scaled.shape[1], n_features))
        temp[:, close_idx] = y_true_scaled[i]
        inv_true = scaler.inverse_transform(temp)[:, close_idx]

        temp[:, close_idx] = y_pred_scaled[i]
        inv_pred = scaler.inverse_transform(temp)[:, close_idx]

        true_1d.append(inv_true[0])
        pred_1d.append(inv_pred[0])

    true_1d = np.array(true_1d)
    pred_1d = np.array(pred_1d)
    mae = float(mean_absolute_error(true_1d, pred_1d))
    rmse = float(np.sqrt(mean_squared_error(true_1d, pred_1d)))
    mean_y = float(np.mean(true_1d))
    r2 = float(r2_score(true_1d, pred_1d)) if len(true_1d) > 1 else None
    return mae, r2, rmse

def forecast_next_days(model, df_raw: pd.DataFrame, scaler: MinMaxScaler, meta: dict, days_ahead: int):
    cols = meta["cols"]
    close_idx = meta["close_idx"]
    lookback = meta["lookback"]
    horizon = meta["horizon"]

    df = add_features(df_raw)
    df_w = df[cols].copy()
    scaled = scaler.transform(df_w.values)

    last_seq = scaled[-lookback:]
    last_seq = np.expand_dims(last_seq, axis=0)  # (1, lookback, n_features)

    # Model always outputs 'horizon' steps; if user asks fewer, slice
    pred_scaled = model.predict(last_seq, verbose=0)[0]  # (horizon,)
    if days_ahead < horizon:
        pred_scaled = pred_scaled[:days_ahead]

    # Inverse transform just for close
    temp = np.zeros((len(pred_scaled), len(cols)))
    temp[:, close_idx] = pred_scaled
    inv = scaler.inverse_transform(temp)[:, close_idx]
    return inv.tolist()

def confidence_from_rmse(metrics: dict, df_raw: pd.DataFrame) -> float:
    """Crude confidence heuristic: higher if RMSE is small relative to mean close."""
    try:
        mean_close = float(df_raw["close"].tail(200).mean())
        rmse = metrics.get("rmse", None)
        if rmse is None or mean_close <= 0:
            return 50.0
        c = 100.0 * (1.0 - (rmse / mean_close))
        return float(np.clip(c, 0.0, 100.0))
    except Exception:
        return 50.0

# ============================================
# Routes
# ============================================
@app.get("/", response_class=HTMLResponse)
@app.head("/")
def home():
    html = """
    <h1>ðŸ“ˆ Stock Price Prediction API (BiLSTM)</h1>
    <form action="/predict" method="get">
        <label>Stock Symbol:</label>
        <input name="stock_symbol" value="TCS.NS"><br><br>
        <label>Days Ahead (1-30):</label>
        <input name="days_ahead" type="number" value="5" min="1" max="30"><br><br>
        <button type="submit">Predict</button>
    </form>
    """
    return HTMLResponse(content=html)

@app.get("/health")
def health():
    return {"status": "ok"}

@app.get("/predict")
def predict(
    stock_symbol: str = Query("TCS.NS", description="Stock ticker symbol"),
    days_ahead: int = Query(5, ge=1, le=30, description="Days ahead to predict"),
    force_retrain: Optional[bool] = Query(False, description="Force retrain even if cached model exists"),
    debug: Optional[bool] = Query(False, description="Include traceback in response for debugging")
):
    logger.info("Predict request: %s, days_ahead=%d, force_retrain=%s", stock_symbol, days_ahead, force_retrain)
    try:
        if not stock_symbol or not isinstance(stock_symbol, str):
            raise ValueError("stock_symbol must be a non-empty string")

        # 1) Fetch data (smart chooser â†’ CSV fallback)
        df = fetch_stock_data(stock_symbol)
        if df is None or df.empty:
            raise ValueError(f"No data returned for {stock_symbol}")

        # 2) Train or load model
        model, scaler, meta, metrics = train_or_load_model(
            symbol=stock_symbol,
            df_raw=df,
            lookback=120,
            horizon=30,
            force_retrain=bool(force_retrain)
        )

        # 3) Forecast
        preds = forecast_next_days(model, df_raw=df, scaler=scaler, meta=meta, days_ahead=days_ahead)

        # 4) Confidence
        conf = confidence_from_rmse(metrics, df)
        conf_list = [round(conf, 2)] * len(preds)

        result = {
            "stock_symbol": stock_symbol,
            "days_ahead": days_ahead,
            "predictions": [float(round(x, 4)) for x in preds],
            "confidence": conf_list,
            "mae": metrics.get("mae"),
            "r2": metrics.get("r2"),
            "rmse": metrics.get("rmse"),
            "model_files": {
                "weights_h5": f"models_tf/{stock_symbol}_bilstm.h5",
                "scaler_pkl": f"models_tf/{stock_symbol}_scaler.pkl"
            }
        }
        return JSONResponse(content=result)

    except Exception as exc:
        tb = traceback.format_exc()
        logger.error("Error: %s\n%s", exc, tb)
        if debug:
            return JSONResponse(status_code=500, content={"error": str(exc), "traceback": tb})
        return JSONResponse(status_code=500, content={"error": str(exc)})

# ===== Run with: uvicorn main:app --reload =====
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import joblib
import nltk

# Download stopwords if needed
nltk.download('stopwords')
from nltk.corpus import stopwords

# ==============================
# Load dataset
# ==============================
# You can use any labeled dataset with 'text' and 'label' columns
# Example: label=1 for positive, 0 for negative
# For demo, we'll create a small sample dataset
data = pd.DataFrame({
    "text": [
        "Stock price soared after great earnings report",
        "Company faces massive losses due to scandal",
        "Profits exceeded expectations this quarter",
        "Market reacts negatively to poor guidance",
        "Shares jump on acquisition news",
        "Product recall causes stock to tumble"
    ],
    "label": [1, 0, 1, 0, 1, 0]
})

X = data["text"]
y = data["label"]

# ==============================
# Split data
# ==============================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==============================
# Build pipeline
# ==============================
pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(stop_words=stopwords.words("english"), ngram_range=(1,2))),
    ("clf", LogisticRegression(solver="liblinear"))
])

# ==============================
# Train model
# ==============================
pipeline.fit(X_train, y_train)

# ==============================
# Test accuracy
# ==============================
acc = pipeline.score(X_test, y_test)
print("Test Accuracy:", acc)

# ==============================
# Save model
# ==============================
joblib.dump(pipeline, "sentiment_model.pkl")
print("âœ… Sentiment model saved as sentiment_model.pkl")

import joblib

# Load model
model = joblib.load("sentiment_model.pkl")

# Example headlines
headlines = [
    "Stock jumps after record profits",
    "Company sued for environmental damage"
]

# Predict sentiment probabilities
probs = model.predict_proba(headlines)[:, 1]  # probability of positive sentiment
print(probs)

# Predict sentiment class (0=negative, 1=positive)
labels = model.predict(headlines)
print(labels)

print(df.columns.tolist())

